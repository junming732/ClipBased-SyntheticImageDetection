#!/usr/bin/env python3
"""
True Zero-Shot CLIP Detection for CIFAKE
Uses text prompts to classify images without any training on fake images
"""
import torch
import open_clip
from PIL import Image
import pandas as pd
from tqdm import tqdm
import numpy as np
from sklearn.metrics import roc_auc_score, accuracy_score, classification_report
import matplotlib.pyplot as plt
import sys

def run_zero_shot_clip(csv_file, model_name='ViT-L-14', pretrained='openai', batch_size=128):
    """
    Test zero-shot CLIP with different text prompts
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # Load CLIP model
    print(f"\nLoading CLIP model: {model_name} ({pretrained})")
    model, _, preprocess = open_clip.create_model_and_transforms(
        model_name, pretrained=pretrained
    )
    model = model.to(device)
    model.eval()

    # Load dataset
    print(f"\nLoading dataset from {csv_file}")
    df = pd.read_csv(csv_file)
    print(f"Total images: {len(df)}")

    # Different text prompt strategies to try
    prompt_strategies = {
        'simple': [
            "a real photograph",
            "an AI-generated image"
        ],
        'detailed': [
            "a real photograph taken with a camera",
            "a synthetic image generated by artificial intelligence"
        ],
        'descriptive': [
            "an authentic natural photograph",
            "a computer-generated fake image"
        ],
        'technical': [
            "a genuine photo from a camera sensor",
            "an artificially synthesized image from a generative model"
        ],
    }

    results = {}

    for strategy_name, prompts in prompt_strategies.items():
        print(f"\n{'='*70}")
        print(f"Testing strategy: {strategy_name.upper()}")
        print(f"Prompts: {prompts}")
        print(f"{'='*70}")

        # Tokenize text prompts
        text_tokens = open_clip.tokenize(prompts).to(device)

        # Get text embeddings
        with torch.no_grad():
            text_features = model.encode_text(text_tokens)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)

        # Process images in batches
        all_probs = []
        all_labels = []

        for i in tqdm(range(0, len(df), batch_size), desc="Processing images"):
            batch_df = df.iloc[i:i+batch_size]

            # Load and preprocess images
            images = []
            labels = []
            for idx, row in batch_df.iterrows():
                try:
                    img = Image.open(row['filename']).convert('RGB')
                    img_tensor = preprocess(img)
                    images.append(img_tensor)
                    labels.append(1 if row['typ'] == 'fake' else 0)
                except Exception as e:
                    print(f"Error loading {row['filename']}: {e}")
                    continue

            if len(images) == 0:
                continue

            # Stack images into batch
            image_batch = torch.stack(images).to(device)

            # Get image embeddings
            with torch.no_grad():
                image_features = model.encode_image(image_batch)
                image_features = image_features / image_features.norm(dim=-1, keepdim=True)

                # Compute similarity to each text prompt
                similarity = (image_features @ text_features.T) * 100  # Scale by 100

                # Softmax to get probabilities
                probs = similarity.softmax(dim=-1)

                # Probability of being "AI-generated" (second prompt)
                fake_probs = probs[:, 1].cpu().numpy()

            all_probs.extend(fake_probs)
            all_labels.extend(labels)

        # Convert to numpy arrays
        all_probs = np.array(all_probs)
        all_labels = np.array(all_labels)

        # Predictions (threshold at 0.5)
        all_preds = (all_probs > 0.5).astype(int)

        # Calculate metrics
        auc = roc_auc_score(all_labels, all_probs)
        accuracy = accuracy_score(all_labels, all_preds)

        results[strategy_name] = {
            'prompts': prompts,
            'auc': auc,
            'accuracy': accuracy,
            'predictions': all_preds,
            'probabilities': all_probs,
            'labels': all_labels
        }

        print(f"\nResults for {strategy_name}:")
        print(f"  ROC AUC:    {auc:.4f}")
        print(f"  Accuracy:   {accuracy:.4f}")
        print("\nClassification Report:")
        print(classification_report(all_labels, all_preds,
                                   target_names=['Real', 'Fake'],
                                   digits=4))

    return results

def plot_comparison(results, output='zero_shot_clip_comparison.png'):
    """Compare different prompt strategies"""
    strategies = list(results.keys())
    aucs = [results[s]['auc'] for s in strategies]
    accs = [results[s]['accuracy'] for s in strategies]

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    # AUC comparison
    bars1 = ax1.bar(strategies, aucs, color='steelblue')
    ax1.set_ylabel('ROC AUC')
    ax1.set_title('Zero-Shot CLIP: Prompt Strategy Comparison (AUC)')
    ax1.set_ylim([0, 1])
    ax1.axhline(y=0.5, color='r', linestyle='--', label='Random')
    ax1.legend()
    ax1.grid(True, alpha=0.3, axis='y')

    for bar in bars1:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}', ha='center', va='bottom')

    # Accuracy comparison
    bars2 = ax2.bar(strategies, accs, color='coral')
    ax2.set_ylabel('Accuracy')
    ax2.set_title('Zero-Shot CLIP: Prompt Strategy Comparison (Accuracy)')
    ax2.set_ylim([0, 1])
    ax2.axhline(y=0.5, color='r', linestyle='--', label='Random')
    ax2.legend()
    ax2.grid(True, alpha=0.3, axis='y')

    for bar in bars2:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}', ha='center', va='bottom')

    plt.tight_layout()
    plt.savefig(output, dpi=300)
    print(f"\n✓ Saved comparison plot: {output}")

if __name__ == '__main__':
    # Get CSV file
    if len(sys.argv) > 1:
        csv_file = sys.argv[1]
    else:
        csv_file = 'cifake_test.csv'

    print("="*70)
    print("TRUE ZERO-SHOT CLIP DETECTION ON CIFAKE")
    print("Using text prompts (no training on fake images)")
    print("="*70)

    # Run zero-shot CLIP
    results = run_zero_shot_clip(csv_file, model_name='ViT-L-14', pretrained='openai')

    # Find best strategy
    best_strategy = max(results.keys(), key=lambda k: results[k]['auc'])
    best_result = results[best_strategy]

    print(f"\n{'='*70}")
    print(f"BEST STRATEGY: {best_strategy.upper()}")
    print(f"{'='*70}")
    print(f"Prompts: {best_result['prompts']}")
    print(f"ROC AUC:    {best_result['auc']:.4f}")
    print(f"Accuracy:   {best_result['accuracy']:.4f}")

    # Plot comparison
    plot_comparison(results)

    print("\n" + "="*70)
    print("✓ Zero-shot CLIP testing complete!")
    print("="*70)
    print(f"\nConclusion:")
    if best_result['auc'] > 0.7:
        print(f"✓ Zero-shot works well (AUC={best_result['auc']:.3f})! May not need fine-tuning.")
    else:
        print(f"✗ Zero-shot is weak (AUC={best_result['auc']:.3f}). Proceed with fine-tuning.")
        print("  Run: python train_cifake_clip.py")